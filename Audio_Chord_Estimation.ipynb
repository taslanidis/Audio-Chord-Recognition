{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Audio Chord Estimation using Neural Networks</h1>\n",
    "<h4>Theofanis Aslanidis</h4>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Abstract</h3>\n",
    "In this project, we are going to explore the use of deep neural networks for recognizing audio chords, using the Isophonics dataset - the one that is used on MIREX.<br>\n",
    "For the first steps of the implementation, I'm using a test-dataset only with one album, because the memory needed is very large, due to the one hot representation. So I'm using only the album \"<i>Let it Be</i>\" until I implement a batch training, where I won't need to load all data on memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#python input/output and regex\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "#signal processing libraries\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "\n",
    "#sklearn for normalization\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Audio File pre-processing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to Convert <b>mp3 files to wav</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "### get all audio files and create spectrogram for each track\n",
    "for filename in Path('Audiofiles/The Beatles').glob('**/mp3/*.mp3'):\n",
    "    # files\n",
    "    path, track = os.path.split(filename)\n",
    "    dst = re.sub(r'mp3', 'wav/', path) + re.sub(r'mp3', 'wav', track)\n",
    "    # convert wav to mp3                                                            \n",
    "    sound = AudioSegment.from_mp3(filename)\n",
    "    #optional line if you want them in mono\n",
    "    sound = sound.set_channels(1)\n",
    "    sound.export(dst, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to Convert files from <b>stereo to mono</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in Path('Audiofiles/The Beatles').glob('**/wav/*.wav'):\n",
    "    # convert stereo to mono                                                            \n",
    "    sound = AudioSegment.from_wav(filename)\n",
    "    sound = sound.set_channels(1)\n",
    "    sound.export(filename, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Test</b> and Plot Created Spectrogram on both techniques (SciPy, matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129, 47854)\n"
     ]
    }
   ],
   "source": [
    "### read wav and create spectogram\n",
    "path = 'Audiofiles/The Beatles/12_-_Let_It_Be/wav/06.Let It Be (Remastered 2009).wav'\n",
    "sample_rate, samples = wavfile.read(path)\n",
    "\n",
    "# use scipy signal for spectrogram\n",
    "#frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "\n",
    "### plot spectogram\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.pcolormesh(times, frequencies, 10*np.log10(spectrogram))\n",
    "powerSpectrum, frequencies, times, imageAxis = plt.specgram(samples, Fs=sample_rate)\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading Datasets</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the <b>isophonics</b> dataset, in the chordlab folder, there are all the annotations we need.<br>\n",
    "Read all those chordlab files as <b>pandas dataframes</b> and store them in a dictionary of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get all chordlab files\n",
    "Chordlab = {'The Beatles': {}}\n",
    "for filename in Path('Test-Dataset/The Beatles').glob('**/chordlab/**/*.lab'):\n",
    "    \n",
    "    path, track = os.path.split(filename)\n",
    "    path, album = os.path.split(path)\n",
    "    track_no = re.search('([0-9].)_-_',track).group(1)\n",
    "    \n",
    "    if (album not in Chordlab['The Beatles']): \n",
    "        Chordlab['The Beatles'][album] = {}\n",
    "        \n",
    "    Chordlab['The Beatles'][album][track_no] = pd.read_csv(filename, names=['Starts', 'Ends', 'Chord'], sep=' ', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the tracks we have the annotations, we need to load the appropriate audio track for each one, create the spectrogram, and store the (time, frequency, power) vector.<br>\n",
    "<b>Steps:</b>\n",
    "<ul>\n",
    "    <li> Browse to those wav files\n",
    "    <li> Read with wavfile from SciPy\n",
    "    <li> Create spectrogram with plt.specgram\n",
    "</ul>\n",
    "<br>\n",
    "<b>Note:</b> For the spectrogram there are 2 ways to go,<br>\n",
    "<i>1) plt.specgram</i> (+) seems fast (-) less detailed<br>\n",
    "<i>2) Scipy signal.spectrogram</i> (+) more detailed (-) slow<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### get all audio files and create spectrogram for each track\n",
    "Spectrograms = {'The Beatles': {}}\n",
    "for filename in Path('Test-Audiofiles/The Beatles').glob('**/wav/*.wav'):\n",
    "    \n",
    "    path, track = os.path.split(filename)\n",
    "    path, wav = os.path.split(path)\n",
    "    path, album = os.path.split(path)\n",
    "    \n",
    "    track_no = re.search('([0-9].).',track).group(1)\n",
    "    \n",
    "    ### read wav and create spectogram\n",
    "    sample_rate, samples = wavfile.read(filename)\n",
    "    frequencies, times, powerSpectrum = signal.spectrogram(samples, fs=sample_rate, nfft=1024, noverlap=128)\n",
    "    \n",
    "    if (album not in Spectrograms['The Beatles']): \n",
    "        Spectrograms['The Beatles'][album] = {}\n",
    "        \n",
    "    Spectrograms['The Beatles'][album][track_no] = {'powerSpectrum' : powerSpectrum, \n",
    "                                                    'frequencies' : frequencies, \n",
    "                                                    'times' : times}\n",
    "\n",
    "# size of frequency dimension\n",
    "frequencies_num = frequencies.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting spectrograms contain info for 513 frequency range with an overlap of 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency range:  513\n"
     ]
    }
   ],
   "source": [
    "print ('Frequency range: ', powerSpectrum.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data preprocessing, convert data into input that I can feed in the neural network</h3>\n",
    "<br>\n",
    "<ol>\n",
    "    <li> For training we should have all anotations, together with their spectrograms'\n",
    "    <li> For testing all the spectrograms of the test dataset\n",
    "</ol>\n",
    "<br>\n",
    "<b>Shape of the final training data</b>\n",
    "<ul>\n",
    "    <li>X_train, Y_train = [spectrograms], [annotations]\n",
    "    <li>X_test = [spectrograms_test]\n",
    "</ul>\n",
    "<br>\n",
    "<b>Steps to convert data from dictionaries into data that I can feed in the neural network</b>\n",
    "<br>\n",
    "Firstly, I'm thinking I have to create a vocabulary for the chords.<br>\n",
    "Such as the <b>word2vec</b> representation we use on NLP, we will need in this project something similar.<br>\n",
    "<b>Chord:</b><br>\n",
    "C -> 0<br>\n",
    "C#\\Db -> 1<br>\n",
    "D -> 2<br>\n",
    "D#\\Eb -> 3<br>\n",
    "E -> 4<br>\n",
    "F -> 5<br>\n",
    "F#\\Gb -> 6<br>\n",
    "etc.<br>\n",
    "<br>\n",
    "<b>Mode:</b>\n",
    "Minor Chord: 0<br>\n",
    "Major Chord: 1<br>\n",
    "<br>\n",
    "<b>Sustained: </b>\n",
    "1,2,3,4,5,6,7,8,9<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it seems, the combinations can vary.<br>\n",
    "So we can:\n",
    "<ul>\n",
    "    <li>try an encoding(hashing) for each chord, and when we stumble upon each chord we hash it to a unique combination that goes into a vector.</li>\n",
    "    <li>we can iterate all train data, and for each chord we see, create a \"slot\" in our dictionary. So we have a dictionary with all the chords that appear on the dataset. That way we can work with a <b>one hot</b> representation.</li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Chords in our Dataset:  68\n"
     ]
    }
   ],
   "source": [
    "### Load all chords in a dictionary\n",
    "Chords = []\n",
    "for album in Chordlab['The Beatles'].keys():\n",
    "    for track_no in Chordlab['The Beatles'][album].keys():\n",
    "        for index, row in Chordlab['The Beatles'][album][track_no].iterrows():\n",
    "            if row['Chord'] not in Chords: Chords.append(row['Chord'])\n",
    "                \n",
    "### How many chords do we have in our dataset?\n",
    "print (\"Unique Chords in our Dataset: \",len(Chords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one album we have 68 Chords, so our vectors are going to be <b>one hot vectors</b> of 68 size. (67 zeros and 1 one)<br>\n",
    "Create one hot encodings of the chords, with <b>pandas Series + get_dummies</b><br>\n",
    "<h4>One Hot Encodings</h4>\n",
    "<br>\n",
    "With <b>pandas</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chords = len(Chords)\n",
    "s = pd.Series(Chords)\n",
    "OneHotEncodings = pd.get_dummies(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With <b>Sklearn</b>:<br>\n",
    "<i>(which provides us with inverse transformation, for the results.)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "              n_values=None, sparse=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encoder.fit(np.array(pd.Series(Chords)).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Indexing chords with timestamps</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Vector_flow.png\" width=\"800\" />\n",
    "\n",
    "<caption><center> <b>Figure 1</b>: Annotation Data Processing.</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to align the <b>timing of the chords</b> with the <b>times array</b><br>\n",
    "To achieve that lets try to convert the timing - chords data of the .lab files in the same representation as the spectrograms data.<br>\n",
    "We will keep the same timeline (times), and using the timing<br>\n",
    "<b><font color='red'>Warning</font>: This is computational expensive for the cpu. If you have it ready, you can load it below with pickle.</b><br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12_-_Let_It_Be : track  03  completed\n",
      "12_-_Let_It_Be : track  07  completed\n",
      "12_-_Let_It_Be : track  10  completed\n",
      "12_-_Let_It_Be : track  12  completed\n",
      "12_-_Let_It_Be : track  08  completed\n",
      "12_-_Let_It_Be : track  01  completed\n",
      "12_-_Let_It_Be : track  04  completed\n",
      "12_-_Let_It_Be : track  11  completed\n",
      "12_-_Let_It_Be : track  02  completed\n",
      "12_-_Let_It_Be : track  05  completed\n",
      "12_-_Let_It_Be : track  09  completed\n",
      "12_-_Let_It_Be : track  06  completed\n"
     ]
    }
   ],
   "source": [
    "chords2vec = {}\n",
    "for album in Spectrograms['The Beatles'].keys():\n",
    "    chords2vec[album] = {}\n",
    "    for track_no in Spectrograms['The Beatles'][album].keys():\n",
    "        times = Spectrograms['The Beatles'][album][track_no]['times']\n",
    "        df_rows = Chordlab['The Beatles'][album][track_no].itertuples()\n",
    "        index = 0\n",
    "        max_len = len(Chordlab['The Beatles'][album][track_no])\n",
    "        vector = np.empty((len(Chords),))\n",
    "        row = next(df_rows)\n",
    "        for timestamp in times:\n",
    "            if ((index + 1) < max_len) & (timestamp >= row[2]):\n",
    "                index += 1\n",
    "                row = next(df_rows)\n",
    "            vector = np.column_stack((vector, encoder.transform([[row[3]]]).toarray()[0]))        \n",
    "        chords2vec[album][track_no] = vector\n",
    "        print (album, ': track ', track_no, ' completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our 4D data ready in a dictionary to go into our model for training.<br>\n",
    "3 dimensions (times, frequencies, power) will go as X_train and chords2vec will go as Y_train<br><br>\n",
    "Take care fo the one extra row in the beggining of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 78633)\n"
     ]
    }
   ],
   "source": [
    "print (chords2vec['12_-_Let_It_Be']['03'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for album in chords2vec.keys():\n",
    "    for track_no in chords2vec[album].keys():\n",
    "        chords2vec[album][track_no] = np.delete(chords2vec[album][track_no],0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Store - Load Data with pickle</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STORE\n",
    "import pickle\n",
    "\n",
    "with open('let_it_Be.pickle', 'wb') as handle:\n",
    "    pickle.dump(chords2vec, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD\n",
    "import pickle\n",
    "\n",
    "with open('let_it_Be.pickle', 'rb') as handle:\n",
    "    chords2vec = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article the following is stated.<br>\n",
    "https://tm3.ghost.io/2018/09/05/building-lstms-for-time-series-forecasting/<br>\n",
    "<i>\"Instead of one sequence of 1870, you could have many sequences of let's say 20. Your sequences should be overlapping windows `[0-20], [1-21], [2-22]`, etc, so your final shape would be something like `(1850, 20, 14)`.Same process for your test data. Break into subsequences of the same length as training. You will have to play around with finding what a good subsequence length is. It is extremely important to have many different ways of slicing your data. If you train on just one super long sequence it will probably not learn anything interesting.\"</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train/Test Data Initialization</h3>\n",
    "<br>\n",
    "After appending all the data together, we slice them into batches, as shown below:<br>\n",
    "(Example is for the X_train, we do exactly the same for the Y_train - chord annotations.)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/slicing.png\" width=\"800\" />\n",
    "\n",
    "<caption><center> <b>Figure 2</b>: Slicing data into Batches.</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1. Constructing numpy arrays</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(713717, 513) (713717, 68)\n"
     ]
    }
   ],
   "source": [
    "x_initial_train = np.zeros((1,frequencies_num))\n",
    "y_initial_train = np.zeros((1,len(Chords)))\n",
    "test_track_no = '07'\n",
    "for album in chords2vec.keys():\n",
    "    for track_no in chords2vec[album].keys():\n",
    "        if track_no != '07':\n",
    "            y_initial_train = np.append(y_initial_train, chords2vec[album][track_no].T, axis = 0)\n",
    "            x_initial_train = np.append(x_initial_train, Spectrograms['The Beatles'][album][track_no]['powerSpectrum'].T, axis = 0)\n",
    "\n",
    "y_initial_test = chords2vec['12_-_Let_It_Be']['07'].T\n",
    "x_initial_test = Spectrograms['The Beatles']['12_-_Let_It_Be']['07']['powerSpectrum'].T\n",
    "\n",
    "print (x_initial_train.shape, y_initial_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>If you have limited RAM clear this 4GB dictionary for memory issues !</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spectrogram dict is no longer necessary\n",
    "Spectrograms.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2. Normalization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# fit scaler on training dataset\n",
    "scaler.fit(x_initial_train)\n",
    "# transform training dataset\n",
    "x_initial_train = scaler.transform(x_initial_train)\n",
    "\n",
    "# fit scaler on training dataset\n",
    "scaler.fit(x_initial_test)\n",
    "# transform training dataset\n",
    "x_initial_test = scaler.transform(x_initial_test)\n",
    "\n",
    "# keras normalization\n",
    "# x_initial_train = keras.utils.normalize(x_initial_train, axis = 0, order = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3. Slicing in batches of timeseries and for the LSTM</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(715, 1000, 513)\n",
      "(715, 1000, 68)\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "\n",
    "# Find the chunk size\n",
    "chunk_size = 1000\n",
    "mod = x_initial_train.shape[1] // chunk_size + 1 #1.000 is the number of timestemps in each batch\n",
    "x_train = np.zeros((1,1000,frequencies_num)) #num of frequencies\n",
    "y_train = np.zeros((1,1000,len(Chords)))\n",
    "\n",
    "timestep = 0\n",
    "while timestep < x_initial_train.shape[0] :\n",
    "    batch_x = np.resize(x_initial_train[timestep:timestep+chunk_size,:], (1, chunk_size, frequencies_num)) #num of frequencies\n",
    "    batch_y = np.resize(y_initial_train[timestep:timestep+chunk_size,:], (1, chunk_size,len(Chords)))\n",
    "    \n",
    "    x_train = np.append(x_train, batch_x, axis = 0)\n",
    "    y_train = np.append(y_train, batch_y, axis = 0)\n",
    "    timestep += chunk_size\n",
    "    \n",
    "print (x_train.shape)\n",
    "print (y_train.shape)\n",
    "\n",
    "#delete first line batch of array because its zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 1000, 513)\n",
      "(15, 1000, 68)\n"
     ]
    }
   ],
   "source": [
    "x_test = np.zeros((1,1000,frequencies_num)) #num of frequencies\n",
    "y_test = np.zeros((1,1000,len(Chords)))\n",
    "\n",
    "timestep = 0\n",
    "while timestep < x_initial_test.shape[0] :\n",
    "    batch_x = np.resize(x_initial_test[timestep:timestep+chunk_size,:], (1, chunk_size, frequencies_num)) #num of frequencies\n",
    "    batch_y = np.resize(y_initial_test[timestep:timestep+chunk_size,:], (1, chunk_size,len(Chords)))\n",
    "    \n",
    "    x_test = np.append(x_test, batch_x, axis = 0)\n",
    "    y_test = np.append(y_test, batch_y, axis = 0)\n",
    "    timestep += chunk_size\n",
    "    \n",
    "print (x_test.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "#delete first line batch of array because its zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the first row from every array because of the append, which left it all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714, 1000, 513)\n",
      "(714, 1000, 68)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.delete(x_train,0,0)\n",
    "y_train = np.delete(y_train,0,0)\n",
    "x_test = np.delete(x_test,0,0)\n",
    "y_test = np.delete(y_test,0,0)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TensorFlow model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define first an RNN with tensorflow<br>\n",
    "- Layers = dimensionality of the output space\n",
    "- input shape = for every time step the feature shape\n",
    "- Dense output shape is the chord num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(chords_num, frequencies_num, batch_size):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(keras.layers.Bidirectional(keras.layers.LSTM(frequencies_num, return_sequences=True)))\n",
    "    model.add(keras.layers.Dropout(0.1))\n",
    "    model.add(keras.layers.Bidirectional(keras.layers.LSTM(frequencies_num, return_sequences=True, activation='sigmoid')))\n",
    "    model.add(keras.layers.Dropout(0.1))\n",
    "    model.add(keras.layers.Dense(chords_num, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 714 samples, validate on 14 samples\n",
      "Epoch 1/15\n",
      "714/714 [==============================] - 383s 537ms/sample - loss: 3.3446 - accuracy: 0.1668 - val_loss: 2.5074 - val_accuracy: 0.3277\n",
      "Epoch 2/15\n",
      "714/714 [==============================] - 375s 526ms/sample - loss: 2.8831 - accuracy: 0.2289 - val_loss: 2.4521 - val_accuracy: 0.2444\n",
      "Epoch 3/15\n",
      "714/714 [==============================] - 374s 524ms/sample - loss: 2.6554 - accuracy: 0.2934 - val_loss: 2.7170 - val_accuracy: 0.1176\n",
      "Epoch 4/15\n",
      "714/714 [==============================] - 374s 523ms/sample - loss: 2.5410 - accuracy: 0.2911 - val_loss: 2.7308 - val_accuracy: 0.2663\n",
      "Epoch 5/15\n",
      "714/714 [==============================] - 376s 526ms/sample - loss: 2.3810 - accuracy: 0.3318 - val_loss: 2.1480 - val_accuracy: 0.4196\n",
      "Epoch 6/15\n",
      "714/714 [==============================] - 373s 523ms/sample - loss: 2.2919 - accuracy: 0.3425 - val_loss: 3.2183 - val_accuracy: 0.0947\n",
      "Epoch 7/15\n",
      "714/714 [==============================] - 373s 522ms/sample - loss: 2.2111 - accuracy: 0.3552 - val_loss: 3.1295 - val_accuracy: 0.1879\n",
      "Epoch 8/15\n",
      "714/714 [==============================] - 372s 521ms/sample - loss: 2.0821 - accuracy: 0.3778 - val_loss: 2.6543 - val_accuracy: 0.2260\n",
      "Epoch 9/15\n",
      "714/714 [==============================] - 373s 522ms/sample - loss: 2.0286 - accuracy: 0.3971 - val_loss: 2.4606 - val_accuracy: 0.3430\n",
      "Epoch 10/15\n",
      "714/714 [==============================] - 373s 522ms/sample - loss: 1.9282 - accuracy: 0.4053 - val_loss: 2.4132 - val_accuracy: 0.3927\n",
      "Epoch 11/15\n",
      "714/714 [==============================] - 372s 522ms/sample - loss: 1.7360 - accuracy: 0.4656 - val_loss: 2.2996 - val_accuracy: 0.3290\n",
      "Epoch 12/15\n",
      "714/714 [==============================] - 373s 522ms/sample - loss: 1.8258 - accuracy: 0.4314 - val_loss: 2.5270 - val_accuracy: 0.2820\n",
      "Epoch 13/15\n",
      "714/714 [==============================] - 373s 522ms/sample - loss: 1.9487 - accuracy: 0.4058 - val_loss: 2.8351 - val_accuracy: 0.1714\n",
      "Epoch 14/15\n",
      "714/714 [==============================] - 373s 522ms/sample - loss: 1.7390 - accuracy: 0.4483 - val_loss: 2.5134 - val_accuracy: 0.2925\n",
      "Epoch 15/15\n",
      "714/714 [==============================] - 373s 522ms/sample - loss: 1.5617 - accuracy: 0.4971 - val_loss: 2.5308 - val_accuracy: 0.3356\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional multiple                  4214808   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection multiple                  6320160   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  69836     \n",
      "=================================================================\n",
      "Total params: 10,604,804\n",
      "Trainable params: 10,604,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "model = RNN(len(Chords), frequencies_num, chunk_size)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train\n",
    "model.fit(x_train, y_train, epochs=epochs,\n",
    "          validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test loss and accuracy : [2.5308258533477783, 0.33564284]\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Final test loss and accuracy :\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transform predictions into Chords</h3>\n",
    "<br>\n",
    "Get the resulting one hot encodings, map to the appropriate chords and write the result<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimated Chords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13995</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13996</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13997</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13998</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13999</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Estimated Chords\n",
       "0                    G\n",
       "1                    G\n",
       "2                    G\n",
       "3                    G\n",
       "4                    N\n",
       "...                ...\n",
       "13995                N\n",
       "13996                N\n",
       "13997                N\n",
       "13998                N\n",
       "13999                N\n",
       "\n",
       "[14000 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_chord_list = []\n",
    "real_chord_list = Chordlab['The Beatles']['12_-_Let_It_Be'][test_track_no]\n",
    "\n",
    "for batch_chords in predictions:\n",
    "    for chord in batch_chords:\n",
    "        estimated_chord_list.append(encoder.inverse_transform([chord]).reshape(1,)[0])\n",
    "        \n",
    "df_predictions = pd.DataFrame({'Estimated Chords' : estimated_chord_list})\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
